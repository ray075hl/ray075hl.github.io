I" <h3 id="符号">符号</h3>

<p><strong>注意:</strong> 在写这段文字的时候, 我理解的 <strong><em>未来奖励</em></strong> = <strong><em>收益</em></strong> = <strong><em>累积回报</em></strong>.</p>

<p><strong>Policy(策略)</strong></p>

<p>用来知道agent如何采取行动</p>

<ul>
  <li>
    <p>A policy is the agent’s behaviour.</p>
  </li>
  <li>
    <p>It is a map from state to action, e.g.</p>
  </li>
  <li>
    <p>Deterministic policy: \(a=\pi(s)\)</p>
  </li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Stochastic policy: $$\pi (a</td>
          <td>s) = p[A_{t}=a</td>
          <td>S_{t}=s]$$</td>
        </tr>
      </tbody>
    </table>

    <p>​</p>
  </li>
</ul>

<p><strong>Value Function(值函数)</strong></p>

<p>用来评价状态的好坏</p>

<ul>
  <li>Value function is a prediction of future reward</li>
  <li>Used to evaluate the goodness/badness of <strong>states</strong></li>
  <li>And therefore to select between actions, e.g.</li>
</ul>

<table>
  <tbody>
    <tr>
      <td>[V_{\pi}(s)=\mathbb E_{\pi}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}r_{t+3}+…</td>
      <td>S_{t}=s]]</td>
    </tr>
  </tbody>
</table>

<p><strong>bellman equation(贝尔曼方程)</strong></p>

<p>由</p>

<table>
  <tbody>
    <tr>
      <td>[V_{\pi}(s)=\mathbb E_{\pi}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}r_{t+3}+…</td>
      <td>S_{t}=s]]</td>
    </tr>
  </tbody>
</table>

<p>推导</p>

<table>
  <tbody>
    <tr>
      <td>[V_{\pi}(s)=\mathbb E_{\pi}[R_{t+1}+\gamma V(s_{t+1})</td>
      <td>S_{t}=s]]</td>
    </tr>
  </tbody>
</table>

<p><strong>Model(模型)</strong></p>

<ul>
  <li>A model predicts what the environment will do next</li>
  <li>P predicts the next state</li>
  <li>R predicts the next (immediate) reward, e.g.</li>
</ul>

<p><strong>Action value function(动作值函数)</strong></p>

<p>用来评价在指定状态下采取各种动作的好坏, 只是在状态s执行特定动作a的好坏<strong>((</strong>后续步骤服从\(\pi(s)\),</p>

<p>是不确定的<strong>))</strong>.</p>

<p>Used to evaluate the goodness/badness of <strong>actions</strong></p>

<p>[Q_{\pi}(s,a)=r_{s,a} + \gamma\sum_{s_{i}}  p_{a,s \rightarrow s_{i}}V(s_{i})]</p>

<p>也可以写成</p>

<p>[Q_{\pi}(s,a)=\sum_{s_{i}} p_{a,s \rightarrow s_{i}}(r_{s,a} + \gamma V(s_{i}))]</p>

<p>这两种写法可以参考下图理解:</p>

<ul>
  <li>
    <p>第一种写法, 执行动作\(a\), 即刻获得立即奖励\(r_{s,a}\) , 再加上后续的未来奖励(<strong>long-term reward</strong>).</p>
  </li>
  <li>
    <p>第二种写法, 认为\(Q_{\pi}(s,a)\) 是 所有可能到达状态通路的奖励之和, 即通路1的收益</p>

    <p>X 通路1的概率 + 通路2的收益 X 通路2的概率 + …</p>
  </li>
</ul>

<p><strong>Bellman update(值函数迭代)</strong></p>

<p>[V_{s} \leftarrow \max_{a} \sum_{s_{i}}p_{a, s \to s_{i}}(r_{s,a}+\gamma V_{s_{i}})]</p>

<p>跟新值函数后, 会影响动作值函数,即:\(Q(S,a)\) 又名action value, 从而影响动作的选择,</p>

<p>即,改变了策略Policy.</p>

<p><img src="../assets/img/mdp2.png" alt="MDP" /></p>

<p>如上图所示, 假设当前状态在\(S\), 可以选择的动作有两个,设为\(a_{1}\), \(a_{2}\) .</p>

<p>假设通过\(a_1\), \(a_2\), 可以到达的状态有四个, 分别记为 \(S_1\),  \(S_2\),  \(S_3\),  \(S_4\).</p>

<p>假设  \(S_1\),  \(S_2\),  \(S_3\),  \(S_4\) 四个状态的值函数已知, 分别为 \(V_1\),  \(V_2\),  \(V_3\),  \(V_4\).</p>

<p>根据 <strong>行为值函数的定义</strong> ,</p>

<p>对于动作 \(a_1\):</p>

<p>[Q_{\pi}(s, a_1)=r_{s, a_1} + \gamma\sum_{s_{i}}  p_{a_1,s \rightarrow s_{i}}V(s^{i})]</p>

<p>对于动作 \(a_2\) :</p>

<p>[Q_{\pi}(s, a_2)=r_{s, a_2} + \gamma\sum_{s_{i}}  p_{a_2,s \rightarrow s_{i}}V(s^{i})]</p>

<p>那么 \(Q_{\pi}(s, a_1)\)  , \(Q_{\pi}(s, a_2)\) 这个两值可以用来评价在状态\(S\) , 执行动作\(a_1\), \(a_2\) 的好坏, 越大就</p>

<p>表示动作越好, 意味着将来的累积回报越大.</p>

<p><strong>Contracting mapping(压缩映射)</strong></p>

<p><strong>定义1</strong>: 设\(X\) 是度量空间,其度量用\(\rho\)表示.映射\(T: X \rightarrow X\),若存在\(a\), \(0 \leq a \lt 1\),使得\(\rho(Tx,Ty) \le a\rho(x,y)\),\(\forall x,y \in X\), 则称\(T\)是\(X\)上的一个压缩映射.</p>

<p>若存在\(x_{0}\in X\)使得\(Tx_{0}=x_{0}\),则称\(x_{0}\)是\(T\)的不动点.</p>

<p><strong>定理1</strong>:完备度量空间上的压缩映射具有唯一的不动点.</p>

<p>定理1是说,从度量空间任意一点出发,只要满足压缩映射,压缩映射的序列必定会收敛到唯一的不动点.因此证明一个迭代序列是不是收敛,只要证明该序列所对应的映射是不是压缩映射.</p>

<p>从当前值函数到下一个迭代值函数的映射可表示为</p>

<p>\(T^{\pi}(v)=R^{\pi}+\gamma P^{\pi}v, 0 \le \gamma \lt 1\)
\(\begin{align}
\rho(T^{\pi}(u),T^{\pi}(v)) =&amp;  \|T^{\pi}(u)-T^{\pi}(v)\|_{\infty} \\
=&amp;  \|(R^{\pi}+ \gamma P^{\pi}u)-(R^{\pi}+\gamma P^{\pi}v)\|_{\infty} \\
=&amp;  \|\gamma P^{\pi}(u-v)\|_{\infty} \\
\le&amp;  \|\gamma P^{\pi}\|u-v\|_{\infty}\|_{\infty} \\
\le&amp;  \gamma \|u-v\|_{\infty}
\end{align}\)
根据定义1,在给定的\(\pi\)下,该序列会收敛.(这里的收敛是指<strong>策略评估</strong>会收敛).</p>

<hr />

<ul>
  <li>**1.为什么要写成\(Q_{\pi}(s, a_1)\)  而不是 \(Q(s, a_1)\) 呢? **</li>
</ul>

<p><strong>答:</strong></p>

<p>\(p_{a_1,s \rightarrow s_{i}}\) 这一项是在策略 \(\pi\) 下的产物, 虽然策略 \(\pi\) 不能决定 \(p_{a_1,s \rightarrow s_{i}}\), 但是有参与.</p>

<table>
  <tbody>
    <tr>
      <td>因为 $$p_{a_1,s \rightarrow s_{i}} = p(a_1</td>
      <td>s) * p(s_{i}</td>
      <td>a_1) $$</td>
    </tr>
  </tbody>
</table>

<table>
  <tbody>
    <tr>
      <td>第一部分由策略\(\pi\) 决定, 后一项 $$p(s_{i}</td>
      <td>a_1)\(即(执行动作\)a_1\(后, 到达状态\)s_i$$的概率), 是由环境ENV</td>
    </tr>
  </tbody>
</table>

<p>决定的.</p>

<hr />

<ul>
  <li><strong>2.Q 和 V 之间的关系是什么?</strong></li>
</ul>

<p><strong>答:</strong></p>

<p>根据定义: \(V_{\pi}(S)\)表示在给定测略\(\pi\)下, agent在状态S处的累积回报的期望, 则:</p>

<table>
  <tbody>
    <tr>
      <td>[V_{\pi}(S)=\sum_{a \in A} \pi(a</td>
      <td>s)Q_{\pi}(s,a)]</td>
    </tr>
  </tbody>
</table>

<p>以上图为例, 则\(A=\{ a_1, a_2 \}\).</p>

<p>[V_{\pi}(S) = \sum_{s_{i}}p_{a, s \to s_{i}}(r_{s,a}+\gamma V_{s_{i}})]</p>

<hr />

<ul>
  <li><strong>3.什么是最优状态值函数?</strong></li>
</ul>

<p><strong>每个策略都对应着一个状态函数,最优策略自然对应着最优状态值函数.</strong></p>

<p>定义: 最优状态值函数\(V^*(s)=\max_{\pi}V_{\pi}(s)\), 最优状态行为值函数\(Q^*(s,a)\),为在所有策略中最大的</p>

<p>状态行为值函数, 即:</p>

<p>[Q^*(s,a)=\max_{\pi}Q_{\pi}(s,a)]</p>

<p>最优状态值函数的贝尔曼方程:</p>

<p>[V^<em>(s)=\max_{a}[r_{s,a} + \gamma \sum_{s_{i} \in S }p_{a, s \rightarrow s_{i}}V^</em>(s_{i})]]</p>

<hr />

<ul>
  <li><strong>4.为什么更新值函数的公式为\(V_{s} \leftarrow \max_{a} \sum_{s_{i}}p_{a, s \to s_{i}}(r_{s,a}+\gamma V_{s_{i}})\) ?</strong></li>
</ul>

<p><strong>答:</strong></p>

<p>假设我们的初始状态为\(S_{0}\), 那么我们的目的是: 通过调整策略\(\pi\)(调整策略,最终会体现在动作\(a\)的选择上),</p>

<p>使得\(V_{0}\)能够最大,即使得状态\(S_{0}\)的累积回报最大. 即:</p>

<p>[V_{S_{0}}=\max_{a \in A} \mathbb E_{s \sim S}[r_{s,a} + \gamma V_{s}]=\max_{a \in A}[\sum p_{a,s_0 \rightarrow s}(r_{s,a}+\gamma V_{s})]]</p>

<p>其中\(S\) 为 \(S_0\) 的后继状态.</p>

<p>上式最右边可以看成是两个阶段,</p>

<ul>
  <li>第一个阶段是\([***]\) 中括号里面的部分, 这是策略评估阶段(<strong>policy evaluate</strong>)</li>
  <li>第二个阶段是\(\max_{a \in A}\) , 这是策略改善阶段(<strong>policy improve</strong>)</li>
</ul>

<p>所以更新式实际上就是<strong>策略迭代方法(policy iterate)</strong>.</p>

:ET