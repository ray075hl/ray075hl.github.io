I"ø&<h2 id="1-tensors">1. Tensors</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="mf">3.14</span><span class="p">)</span>  <span class="c1"># a scalar is an zero-dimensional tensor
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span> <span class="c1"># vector with size 10
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> <span class="c1"># tensor with 2 rows and 3 cols
</span><span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">)</span> <span class="c1"># a 3 dimensional tensor 
</span>
<span class="c1"># Initialize a tensor with list or tuple
</span><span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">n</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span><span class="mi">2</span><span class="p">))</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="c1"># torch.from_numpy(n) method will be deprecated
</span>
<span class="c1"># GPU tensor
</span><span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="s">"cuda"</span> <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">is_available</span><span class="p">()</span> <span class="k">else</span> <span class="s">"cpu"</span><span class="p">)</span>
<span class="n">b</span> <span class="o">=</span> <span class="n">b</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span> <span class="c1"># if device is cuda
</span></code></pre></div></div>

<h2 id="2-gradients">2. Gradients</h2>

<p><strong>Static graph</strong>: The graph gets processed and optimized by the DL library before any computation can be made.</p>

<p><strong>Dynamic graph</strong>: The library records the order of operations performed, and when you ask it to calcute gradients, it unrolls its history of operations, accumulating the gradients of network parameters. This method is also called <strong>notebook gradients</strong>.</p>

<h2 id="3-nn-building-blocks">3. NN building blocks</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="n">nn</span>
<span class="n">s</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
</code></pre></div></div>

<h2 id="4-custom-layers">4. Custom layers</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">OurModule</span><span class="p">(</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
  <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">.</span> <span class="n">num_inputs</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dropout_prob</span><span class="o">=</span><span class="mf">0.3</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">OurModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">).</span><span class="n">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="p">.</span><span class="n">pipe</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="mi">20</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">ReLU</span><span class="p">(),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout_prob</span><span class="p">),</span>
        <span class="n">nn</span><span class="p">.</span><span class="n">Softmax</span><span class="p">()</span>
    <span class="p">)</span>
  <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="p">.</span><span class="n">pipe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="p">:</span>
  <span class="n">net</span> <span class="o">=</span> <span class="n">OurModule</span><span class="p">(</span><span class="n">num_inputs</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">num_classes</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
  <span class="n">v</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">FloatTensor</span><span class="p">([[</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">]])</span>
  <span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
  <span class="k">print</span><span class="p">(</span><span class="n">net</span><span class="p">);</span> <span class="k">print</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
</code></pre></div></div>

<h2 id="5-loss-functions">5. Loss functions</h2>

<p>Pytorch 0.4 contains 17 different lossfunctions. For example:</p>

<ul>
  <li>
    <p>nn.MSELoss</p>
  </li>
  <li>
    <p>nn.BCELoss</p>
  </li>
  <li>
    <p>nn.CrossEntropyLoss</p>

    <p>â€‹</p>
  </li>
</ul>

<h2 id="6-optimizers">6. Optimizers</h2>

<p>In the torch.optim package, PyTorch provides lots of popular optimizer implementations and</p>

<p>the most widely known are as follows:</p>

<ul>
  <li>SGD: A vanilla stochastic gradient descent algorithm with optional momentum extension</li>
  <li>RMSprop: An optimizer, proposed by G. Hinton</li>
  <li>Adagrad: An adaptive gradients optimizer</li>
</ul>

<h2 id="7-monitoring-with-tensorboard">7. Monitoring with TensorBoard</h2>

<p>pip install tensorboard-pytorch</p>

<hr />

:ET