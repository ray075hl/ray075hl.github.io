I"£<h2 id="policy-gradients">Policy Gradients</h2>

<h3 id="why-policy"><span style="color:red">Why policy</span></h3>

<blockquote>
  <p><strong>REINFORCE METHOD</strong></p>

  <ol>
    <li>
      <p>Initialize the network with random weights</p>
    </li>
    <li>
      <p>Play N full episodes, saving their \((s, a, r, s')\) transitions</p>
    </li>
    <li>
      <p>For every step t of every episode k, calculate the discounted total</p>
    </li>
  </ol>

  <p>reward for subsequent steps \(Q_{k, t}=\sum_{i=0} \gamma^{i}r_{i}\)  .</p>

  <ol>
    <li>
      <p>Calculate the loss function for all transitions \(L=-\sum_{k,t}Q_{k,t}\log(\pi(s_{k,t}, a_{k,t}))\)</p>
    </li>
    <li>
      <p>Perform SGD update of weights minimizing the loss</p>
    </li>
    <li>
      <p>Repeat from step 2 until converged</p>
    </li>
  </ol>
</blockquote>

<p><span style="color:blue"><strong>Different form Q-learning</strong></span></p>

<ul>
  <li>
    <p>No explicit exploration is needed. In Q-learning, we used an epsilon-greedy</p>

    <p>strategy to explore the environment and prevent our agent from getting stuck</p>

    <p>with non-optimal policy. Now, with probabilities returned by the network, the</p>

    <p>exploration is performed automatically. In the beginning, the network is initialized</p>

    <p>with random weights and the network returns uniform probability distribution.</p>

    <p>This distribution corresponds to random agent behavior.</p>
  </li>
  <li>
    <p>No replay buffer is used. PG methods belong to the on-policy methods class,</p>

    <p>which means that we canâ€™t train on data obtained from the old policy. This is</p>

    <p>both good and bad. The good part is that such methods usually converge faster.</p>

    <p>The bad side is they usually require much more interaction with the environment</p>

    <p>than off-policy methods such as DQN.</p>
  </li>
  <li>
    <p>No target network is needed. Here we use Q-values, but theyâ€™re obtained from</p>

    <p>from our experience in the environment. In DQN, we used the target network to</p>

    <p>break the correlation in Q-values approximation, but weâ€™re not approximating it</p>

    <p>anymore.</p>
  </li>
</ul>

<h3 id="policy-based-versus-value-based-methods"><span style="color:red">Policy-based versus value-based methods</span></h3>

<ul>
  <li>
    <p>Policy methods are directly optimizing what we care about: out behavior. The</p>

    <p>value methods such as DQN are doing the same indirectly, learning the value</p>

    <p>first and providing to us policy based on this value.</p>
  </li>
  <li>
    <p>Policy methods are on-policy and require fresh samples from the environment.</p>

    <p>The value methods can benefit from old data, obtained from the old policy, human</p>

    <p>demonstration, and other sources.</p>
  </li>
  <li>
    <p>Policy methods are usually less sample-efficient, which means they require more</p>

    <p>interaction with the environment. The value methods can benefit from the large</p>

    <p>replay buffers. However, sample efficiency doesnâ€™t mean that value methods are</p>

    <p>more computationally efficient and very often itâ€™s the opposite. In the above example,</p>

    <p>during the training, we need to access our NN only once, to get the probabilities of</p>

    <p>actions. in DQN, we need to process two batch of states:  one for the current state and</p>

    <p>another for the next state in the Bellman update.</p>
  </li>
</ul>

<h3 id="limits-of-reinfoce-method"><span style="color: red">Limits of REINFOCE method</span></h3>

<ul>
  <li>
    <p><span style="color:blue">Need full episode</span> to complete before we can start training.need more</p>

    <p>episodes used for training. Itâ€™s a monte carlo sample!  so it situation is</p>

    <p>fine for short episodes in the CarPole.</p>
  </li>
  <li>
    <p><span style="color:blue">High gradients variance</span>. To overcome this, we can subtracting a value called</p>

    <p>baseline from the \(Q\). The possible choices of the baseline are as follows:</p>

    <ul>
      <li>Some constant value, which normally is the mean of the discounted rewards</li>
      <li>The moving average of the discounted rewards</li>
      <li>Value of the state \(V(s)\)</li>
    </ul>
  </li>
  <li>
    <p><span style="color:blue">Exploration</span>. In DQN we solved this using soft-epsilon action selection.</p>

    <p>solution: add a entropy loss to punish the absolutely decision</p>

    <table>
      <tbody>
        <tr>
          <td>$$ H(\pi) =  -\sum \pi(a</td>
          <td>s) \log\pi(a</td>
          <td>s)\(when policy is uniform(random), the\)H(\pi)$$ have</td>
        </tr>
      </tbody>
    </table>

    <p>maximum value.</p>
  </li>
  <li>
    <p><span style="color:blue">Correlation between samples</span>. To solve this problem, <strong>parallel environments</strong> are</p>

    <p>normally used. we use several and use their transitions as training data.</p>
  </li>
</ul>

:ET