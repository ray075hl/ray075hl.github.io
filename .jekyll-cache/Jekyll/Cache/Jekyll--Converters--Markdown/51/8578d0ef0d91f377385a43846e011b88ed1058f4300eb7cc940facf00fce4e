I"«<h4 id="æ•´ä½“ç»“æ„">æ•´ä½“ç»“æ„</h4>

<p>Transformer æºè‡ªâ€Attention is all you needâ€ä¸€æ–‡.</p>

<p>ä¸»è¦çš„æ¨¡å‹ç»“æ„å¦‚ä¸‹:</p>

<p><img src="../assets/img/trans1.jpeg" alt="" /></p>

<p>æ•´ä½“æ¥çœ‹å°±æ˜¯encoder-decoderçš„ç»“æ„, åªä¸è¿‡æ²¡æœ‰ä½¿ç”¨RNNè¿™ç§æ¨¡å—, å–è€Œä»£ä¹‹çš„æ˜¯self-attentionå’Œdenseçš„ç»„åˆæ¨¡å—. å…¶ä¸­åœ¨encoderç«¯å †å äº†6ä¸ªè¯¥æ¨¡å—, decoderç«¯å †å äº†6ä¸ª mask-attention(context attention), self-attention, denseçš„ç»„åˆæ¨¡å—.</p>

<p>dense(æ–‡ä¸­ç§°ä¸º<strong>Position-wise Feed-Forward network</strong>)çš„å…¬å¼ä¸º</p>

\[FFN(x) = relu(xW_{1}+b_{1})W_{2}+b_{2}\]

<h4 id="åˆ†è§£æ¨¡å—">åˆ†è§£æ¨¡å—</h4>

<ul>
  <li>
    <p>Attention module</p>

    <p>åœ¨æ–‡ç« ä¸­æœ‰ä¸‰å¤„å‡ºç°äº†attention, encoderå’Œdecoderä¸­çš„self-attention; decoderä¸­çš„context attention.</p>
  </li>
  <li>
    <p>Multi-head æœºåˆ¶</p>

    <p><img src="../assets/img/multihead.png" alt="" /></p>
  </li>
</ul>

<p>â€‹         è¯¥æ“ä½œå°±æ˜¯å°†ä¸€ä¸ªé•¿çš„featureå‡åŒ€çš„åˆ†ä¸ºnæ®µåˆ†åˆ«æ¥åšscaled Dot-Product Attention.</p>

<ul>
  <li>
    <p>Add &amp; Norm</p>

    <p>Add å®é™…ä¸Šæ˜¯ä¸€ä¸ªresidualè¿æ¥, Normåœ¨è¿™é‡Œé‡‡ç”¨çš„æ˜¯Layer Normalization(ä¸åŒä¸Batch Normalization).</p>
  </li>
  <li>
    <p>Mask</p>

    <p>æ–‡ä¸­çš„Mask åˆ†ä¸ºä¸¤ç§, ä¸€ç§æ˜¯Padding mask, å¦ä¸€ç§æ˜¯Sequence mask.</p>

    <p>Padding maskæ˜¯ç”¨æ¥è¡¥é½æ•°æ®çš„(å› ä¸ºå¥å­çš„é•¿åº¦ä¸ä¸€ç›¸åŒ, çŸ­çš„å¥å­éœ€è¦ç”¨0è¡¥é½);</p>

    <p>Sequence maskæ˜¯ä¸ºäº†ä½¿çš„decoderä¸èƒ½çœ‹è§æœªæ¥çš„ä¿¡æ¯, å³åœ¨time_stepä¸ºtçš„æ—¶åˆ», è§£ç è¾“å‡ºåªèƒ½ä¾èµ–äºtæ—¶åˆ»ä¹‹å‰çš„è¾“å‡º.</p>
  </li>
  <li>
    <p>Positional encoding</p>

    <p>å°†ä½ç½®ä¿¡æ¯ç¼–ç çš„æ¨¡å—, æ–‡ä¸­å°†ä½ç½®ä¿¡æ¯ç¼–ç æˆé•¿åº¦ä¸º512çš„å‘é‡,</p>

    <p>å…¬å¼ä¸º:</p>

\[PE(pos, 2i) = sin(pos/10000^{2i/d_{model}})\]

\[PE(pos,2i+1)=cos(pos/10000^{2i/d_{model}})\]

    <p>ä»£ç å¦‚ä¸‹:</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="n">d_model</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">max_seq_len</span> <span class="o">=</span> <span class="mi">100</span>
  
<span class="n">position_encoding</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([[</span><span class="n">pos</span> <span class="o">/</span> <span class="n">np</span><span class="p">.</span><span class="nb">pow</span><span class="p">(</span><span class="mi">10000</span><span class="p">,</span> <span class="mf">2.0</span><span class="o">*</span><span class="p">(</span><span class="n">j</span><span class="o">//</span><span class="mi">2</span><span class="p">)</span><span class="o">/</span><span class="n">d_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span>     	<span class="nb">range</span><span class="p">(</span><span class="n">d_model</span><span class="p">)]</span> <span class="k">for</span> <span class="n">pos</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_seq_len</span><span class="p">)])</span>
  
<span class="n">position_encoding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position_encoding</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
<span class="n">position_encoding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position_encoding</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">])</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Word Embedding</p>

    <p>ç•¥</p>
  </li>
</ul>

<p>æ›´å…·ä½“çš„ç»†èŠ‚å¯ä»¥é˜…è¯»<a href="https://github.com/jadore801120/attention-is-all-you-need-pytorch">https://github.com/jadore801120/attention-is-all-you-need-pytorch</a>.</p>
:ET