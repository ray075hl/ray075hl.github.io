I"<h2 id="基本概念与符号">基本概念与符号</h2>

<h4 id="策略"><em>策略</em></h4>

<p>策略是静态的，与时间无关，是一个概率（在状态S下执行动作a的概率）</p>

\[\Sigma(a|s) = p[A_t=a|S_t=s] \tag {1}\\\]

<h3 id="奖励函数"><em>奖励函数</em></h3>

<p>即在给定策略 \(\pi{}\) 时的累积回报。</p>

\[G_t=\sum_{k=0}^{\infty}r^kR_{t+k+1}\tag  {2}\]

<h4 id="状态-值函数"><em>状态-值函数</em></h4>

<p>给定策略 \(\pi{}\) ，累积回报在状态s处的期望值 定义为状态-值函数。</p>

\[v_{\pi}(s)=E_{\pi}[\sum_{k=0}^{\infty}r^kR_{t+k+1}|S_t=s] \tag         {3}\]

<h4 id="行为-值函数"><em>行为-值函数</em></h4>

<p>给定策略 \(\pi{}\) ，累积回报在状态s处执行动作a的期望值 定义为行为-值函数</p>

\[q_\pi(s,a)=E_\pi[\sum_{k=0}^{\infty}r^kR_{t+k+1}|S_t=s,A_t=a] \tag   {4}\]

<h4 id="状态-值函数的贝尔曼方程"><em>状态-值函数的贝尔曼方程</em></h4>

<p>\(\begin{split} 
v(s)&amp;=E[G_t|S_t=s] \\ 
&amp;=E[R_{t+1}+rR_{t+2}+...|S_t=s] \\ 
&amp;=E[R_{t+1}+r(R_{t+2}+rR_{t+3}+...)|S_t=s] \\
&amp;=E[R_{t+1}+rG_{t+1}|S_{t}=s] \\
&amp;=E[R_{t+1}+rv(S_{t+1})|S_t=s] 
\end{split}\tag   {5}\)</p>
<h4 id="状态-动作值函数的贝尔曼方程"><em>状态-动作值函数的贝尔曼方程</em></h4>

\[q_{\pi}(s,a)=E_{\pi}[R_{t+1}+rq(S_{t+1},A_{t+1})|S_t=s,A_t=a] \tag   {6}\]

<p>贝尔曼方程的意义在于，把问题的求解转化为递归的求解其子问题，这是运用动态规划的必要条件。</p>

:ET