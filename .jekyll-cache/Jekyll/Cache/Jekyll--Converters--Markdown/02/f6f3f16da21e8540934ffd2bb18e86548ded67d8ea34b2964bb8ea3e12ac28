I"<p><strong>1.1 强化学习可以解决什么问题?</strong></p>

<p>序贯决策问题, 什么是序贯决策问题呢? 就是需要连续不断地作出决策,才能实现最终的目标.</p>

<p>例如一个倒立摆系统, 我们需要根据摆的状态来决定向左还是向右来施加力量,力量的大小是</p>

<p>多少 , 围棋也是,需要根据对手下的棋, 来作出回应.</p>

<p><img src="../assets/img/cartpole.gif" alt="" /></p>

<p><img src="../assets/img/alphago.jpg" alt="" /></p>

<p><strong>1.2 强化学习如何解决问题?</strong></p>

<p>强化学习要解决的是序贯决策问题, 它不关心输入长什么样, 只关心当前输入下应该采用什么</p>

<p>样的动作才能实现最终的目标, 也就说当前采用什么动作,可以使得整个任务序列达到最优. 这就</p>

<p>需要智能体(agent)与环境不断地交互,不断尝试,因为一开始agent也不知道在当前状态下采取</p>

<p>什么动作有利于实现目标.</p>

<p>Agent和环境进行交互时, 环境会返给agent一个当前回报(<span style="color:red"><strong>立即奖励</strong></span>), agent会根据这些奖励来评估</p>

<p>所采取的动作, 有利于实现目标的动作被保留, 不利于的被衰减.</p>

<p><strong>1.3 rl和一般的识别感知问题有什么关系?</strong></p>

<p>就拿我们比较熟悉的OCR问题来说吧, 我们的<strong>OCR属于监督学习, 需要大量多样化的标签数据,而强化学习</strong></p>

<p><strong>需要的是带回报的交互数据</strong>,这是不同点.  相同点是都需要大量的数据.</p>

<p>强化学习的基本框架: <img src=".././assets/img/rl_framework.png" alt="" /></p>

<p>图中的state(\(S_{t}\)), 可以理解成是观测到的量(observation), 例如在cartpole环境中, 它是一个1x4的向量,</p>

<p>在atari游戏里, 它是游戏的画面(一个二维向量).</p>

<p><strong>1.4 强化学习算法的分类</strong></p>

<ul>
  <li>
    <p>根据是否知道模型可以分为基于模型的强化学习算法和无模型(model free)的强化学习算法.</p>

    <table>
      <tbody>
        <tr>
          <td>模型是什么呢? <strong>模型就是状态转移概率</strong>, $$P(S_{t+1}</td>
          <td>S_{t}, a)$$.基于模型的强化学习算法利用与环境交互</td>
        </tr>
      </tbody>
    </table>

    <p>得到的数据学习,在基于模型进行序贯决策. 无模型的强化学习算法则是直接利用与环境交互获得</p>

    <p>的数据改善自身(agent)的行为. 两类方法个有优缺点, 一般来讲居于模型的强化学习算法效率要比</p>

    <p>无模型的强化学习算法效率高(这不是废话吗, 都知道环境怎么运作的,能不高效吗),因为agent在探</p>

    <p>索环境时可以利用模型的信息.<strong>但是,有些根本无法建立模型的任务只能利用无模型的强化学习算法</strong>,</p>

    <p>由于无模型的强化学习算法不需要建模, 所以和基于有模型的算法相比, 更具有通用性, 当前的深度</p>

    <p>强化学习也基本上属于这个分支.</p>
  </li>
  <li>
    <p>根据策略的跟新和学习方法,强化学习可分为基于值函数的强化学习算法和直接基于策略的强</p>

    <p>化学习算法, 以及综合二者的actor-critic方法.</p>
  </li>
</ul>

<p>等等.</p>

<p>借用蝌蝌上周ppt里的图来说明:smile:</p>

<p><img src=".././assets/img/rl_class.png" alt="" /></p>

<p><strong>1.5 强化学习中的一些要素</strong></p>

<p><strong>智能体(Agent)</strong>和<strong>环境(Environment)</strong>, 以及他们之间进行交互时的要素: <strong>动作(Actions)</strong>, <strong>奖励(Reward)</strong>,</p>

<p><strong>状态(Observation or State)</strong>.</p>

<ul>
  <li>
    <p>奖励: 如果我们的agent是一个自动交易程式, 那么它的奖励应该是它作出”买”和”卖”动作时获得的收益.</p>

    <p>​	  如果我们的agent是一个下棋程式, 那么它的奖励只有到棋局结束时才能获得, win or lose.</p>
  </li>
  <li>
    <p>Agent: 自动交易程式, 下棋程式,  或是你自己</p>
  </li>
  <li>环境: 股市, 棋盘的规则</li>
  <li>动作: 买卖, 选一个棋子走了一步, 自己咬自己一口</li>
  <li>状态: 当前的交易情形, 棋子的布局,  你能获得的所有信息</li>
</ul>

<p><strong>策略(policy)</strong></p>

<p>策略是静态的, 如果确定了,就不会随时间变化, 是一个概率(在状态S下执行动作a的概率)</p>

<table>
  <tbody>
    <tr>
      <td>[\pi(a</td>
      <td>s) = p[A_t=a</td>
      <td>S_t=s]]</td>
    </tr>
  </tbody>
</table>

<p><strong>累计回报</strong></p>

<p>给定一个策略\(\pi\)时, 假设从状态\(s_1​\)出发,</p>
:ET