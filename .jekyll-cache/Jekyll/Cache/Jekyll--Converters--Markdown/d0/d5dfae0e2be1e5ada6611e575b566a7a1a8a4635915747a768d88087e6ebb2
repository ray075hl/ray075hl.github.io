I"ÿ<h2 id="value-of-state">Value of state</h2>

<p>In a formal way, the value of the state is: \(V(s) = \mathbb E[\sum_{t=0}^{\infin} r_{t}\gamma^{t}]\) ,  where \(r_{t}\) is the local reward obtained at the step \(t\) of the episode.</p>

<h2 id="the-bellman-equation">The Bellman equation</h2>

<p>Using a recursive format define the value of state.</p>

\[V_{s^{*}}(a)=\mathbb E_{s \sim S}[r_{s, a} + \gamma V_{s}]=\sum_{s \in S} p_{a, s^{*} \to s} (r_{s,a} + \gamma V_{s})\]

<p><strong>Bellman optimality equation</strong></p>

<p>By combining the Bellman equation, for a deterministic case, with a value for stochastic actions, we get the Bellman optimality equation for a general case:</p>

\[V_{s^{*}}(a)=\max_{a \in A}\mathbb E_{s \sim S}[r_{s, a} + \gamma V_{s}]=\max_{a \in A} \sum_{s \in S} p_{a, s^{*} \to s} (r_{s,a} + \gamma V_{s})\]

<p>The optimal value of state is equal to the action, which gives us the maximum possible expected immediate reward plus discounted long-term reward for the next state.</p>

<h2 id="value-of-action">Value of action</h2>

<p>Itâ€™s not as fundamental as value, but we need it for our convenience. Value of action \(Q_{s, a}\). Basically, it equals the <strong>total reward</strong> we can get by executing action \(a\) in state \(s\) and can be defined via \(V_{s}\).  It is slightly more convenient in practice. In these methods, our primary objective is to get values of \(Q\) for every pair of state and action.</p>

\[Q_{s,a}=\mathbb E_{s^{*}\sim S}[r_{s,a}+\gamma V_{s^{*}}]=\sum_{s^{*}\in S}p_{a,s \to s^{*}}{r_{s,a}+\gamma V_{s^{*}}}\]

<p>\(s\) is current state, \(s^{*}\) is next state.</p>

<p>\(Q\) for this state s and action \(a\) equals the expected immediate reward and the discounted long-term reward of the destination state. We also can define \(V_{s}\) via \(Q_{s,a}\):</p>

\[V_{s}=\max_{a\in A}Q_{s,a}\]

<p>This just means that the value of some state equals to the value of the maximum action we can execute from this state. we can also express \(Q(s, a)\) via itself.</p>

\[Q(s,a)=r_{s,a}+\gamma \max_{a^* \in A}Q(s^*,a^*)\]

<p>\(s\) is current state, \(s^{*}\) is next state.</p>

<h2 id="the-value-iteration-method">The Value iteration method</h2>

<p>The procedure (for values of states) includes the following steps:</p>

<ol>
  <li>
    <p>Initialize values of all states \(V_{i}\) to some initial value (usually zero)</p>
  </li>
  <li>
    <p>For every state \(s\) in the <strong>MDP</strong>, perform the Bellman update:</p>

\[V_{s} \leftarrow \max_{a} \sum_{s^{*}}p_{a, s \to s^{*}}(r_{s,a}+\gamma V_{s^{*}})\]
  </li>
  <li>
    <p>Repeat step 2 for some large number of steps or until changes become too small</p>
  </li>
</ol>

<p>In the case of action values (that is Q):</p>

<ol>
  <li>
    <p>Initialize all \(Q_{s,a}\) to zero</p>
  </li>
  <li>
    <p>For every state s and every action a in this state,  perform update:</p>

\[Q_{s,a} \leftarrow \sum_{s^{*}}p_{a, s \to s^{*}}(r_{s,a} + \gamma\max_{a^{*}}Q_{s^{*},a^{*}})\]
  </li>
  <li>
    <p>Repeat step 2</p>
  </li>
</ol>

:ET