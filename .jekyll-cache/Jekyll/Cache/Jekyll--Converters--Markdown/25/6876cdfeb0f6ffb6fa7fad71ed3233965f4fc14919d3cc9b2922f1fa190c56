I"¿<h2 id="1-markov-process">1. Markov process</h2>

<h3 id="11-markov-property">1.1 Markov property</h3>

<p>To call such a system a MP, it needs to fulfil the Markov property,<br />
which means that the future system dynamics from any state have  <br />
to depend on this state only.</p>

<h3 id="12-transition-matrix">1.2 Transition matrix</h3>

<table>
  <thead>
    <tr>
      <th>¬†</th>
      <th>sunny</th>
      <th>rainy</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>sunny</td>
      <td><center>0.8</center></td>
      <td><center>0.2</center></td>
    </tr>
    <tr>
      <td>rainy</td>
      <td><center>0.1</center></td>
      <td><center>0.9</center></td>
    </tr>
  </tbody>
</table>

<p>In practice, we rarely have the luxury of knowing the exact transition <br />
matrix. A much more real-world situation is when we have only observations <br />
of our systems‚Äôs states, which are also called episodes:</p>

<ul>
  <li>sunny -&gt; sunny -&gt; rainy -&gt; sunny</li>
  <li>rainy -&gt; sunny -&gt; sunny</li>
  <li>sunny -&gt; rainy -&gt; rainy -&gt; rainy -&gt; rainy</li>
</ul>

<p>It‚Äôs not complicated to estimate the transition matrix by our observation;<br />
we just count all the transitions from every state and normalize them to a<br />
sum of 1. The more observation data we have, the closer our estimation will<br />
be to the true underlying model.</p>
<h3 id="13-formal-definition-of-markov-process">1.3 Formal definition of Markov process</h3>

<ul>
  <li>A set of state (<strong><em>S</em></strong>) that a system can be in.</li>
  <li>A transition matrix (<strong><em>T</em></strong>), with transition probabilities,  <br />
which defines the system dynamics.</li>
</ul>

<h2 id="2-markov-reward-processmrp">2. Markov reward process(MRP)</h2>

<p>Add value to our transition from state to state. This value can be positive 
or negtive, large or small-it‚Äôs just a number. This number we call it<br />
‚Äúreward‚Äù.</p>

<p><strong>Discount factor</strong> \(\gamma{}\) (gamma), a single number from 0 to 1(inclusive).</p>

<p>For every episode, we define <strong>return</strong> at the time t as this quantity:<br />
\(G_{t}=R_{t+1} + \gamma R_{t+2}+...=\sum_{k=0}^{\infty}r^{k}R_{t+k+1}\)</p>

<p>If \(\gamma{}\) equals to 1, then return \(G_{t}\) just equals a sum of all subsequent rewards, consider</p>

<p>more about futures. If \(\gamma{}\) equals 0,</p>

<p>our return \(G_{t}\) will be just immediate reward without any subsequent<br />
state and correspond to absolute short-sightedness.</p>

<p>\(G_{t}\) is not very useful in practive, as it was defined for every specific<br />
chain we observed from our Markov reward process, so it can vary widely even<br />
for the same state. However, if we go to the extremes and calculate the <br />
mathematical expectation of return for any state (by averaging large amount of<br />
chains), we‚Äôll get a much more useful quantity, called a <strong>value of state</strong>:</p>

<table>
  <tbody>
    <tr>
      <td>[V(s)=E[G</td>
      <td>S_{t}=s]]</td>
    </tr>
  </tbody>
</table>

<h2 id="3-markov-decision-process">3. Markov decision process</h2>

<p>To turn MRP into an MDP, we need to add actions to our reward matrix in the same way we</p>

<p>did with the transition matrix: our reward matrix will depend not only state but also on action.</p>

<p>In other words, it means that the reward the agent obtains now depends not only no the state</p>

<p>it ends up in but also on the action that leads to this state.</p>

<h3 id="31-policy">3.1 Policy</h3>

<p>The intuitive definition of policy is that it is some set of rules that controls the agent‚Äôs behavior.</p>

<p>Policy is defined as the probability distribution over actions for every possible state:</p>

<table>
  <tbody>
    <tr>
      <td>[\pi(a</td>
      <td>s)=P[A_{t}=a</td>
      <td>S_{t}=s]]</td>
    </tr>
  </tbody>
</table>

<p>This is defined as probability, not as a concrete action, to introduce randomness into an agent‚Äôs</p>

<p>behavior.</p>
:ET