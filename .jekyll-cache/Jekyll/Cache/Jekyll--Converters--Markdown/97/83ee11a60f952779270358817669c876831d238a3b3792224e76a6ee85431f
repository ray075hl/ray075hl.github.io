I"P
<h2 id="taxonomy-of-rl-methods">Taxonomy of RL methods</h2>

<ul>
  <li>
    <p>Model-free or model -based</p>
  </li>
  <li>
    <p>Value-based or policy-based</p>
  </li>
  <li>
    <p>On-policy or off-policy</p>

    <p>​</p>
  </li>
</ul>

<p>The term model-free means that the method doesn’t build a model of the environment or reward; it just directly connects observations to actions (or values that are related to actions). In contrast, model-based methods try to predict what the next observation and/or reward will be.</p>

<p>Policy-based methods are directly approximating the policy of the agent, that is, what actions the agent should carry out at every step. Policy is usually represented by probability distribution over the available actions. In contrast, the method could be value-based. In this case, instead of the probability of actions, the	agent calculates the value of every possible action and chooses the action with the best value.</p>

<p>Off-policy as the ability of the method to learn on old historical data (obtained by a previous version of the agent or recorded by human demonstration or just seen by the same agent several episodes ago). On-policy: it requires fresh data obtained from the environment.</p>

<hr />

<p>The core of the cross-entropy method is to throw away bad episodes and train on better ones.</p>

<p><strong>Limitations of cross-entropy method:</strong></p>

<ul>
  <li>
    <p>For training, our episodes have to be finite and, preferably, short</p>
  </li>
  <li>
    <p>The total reward for the episodes should have enough variability to separate good episodes from bad ones</p>
  </li>
  <li>
    <p>There is no intermediate indication about whether the agent has succeeded or failed</p>

    <p>​</p>
  </li>
</ul>

<h2 id="theoretical-background-of-the-cross-entropy-method">Theoretical background of the cross-entropy method</h2>

<p>[E_{x-p(x)}[H(x)]=\int_{x}p(x)H(x)dx=\int_{x}q(x)\frac{p(x)}{q(x)}H(x)dx=E_{x-q(x)}[\frac{p(x)}{q(x)}H(x)]]</p>

<p>In our RL case, H(x) is a reward value obtained by some policy x and p(x) is a distribution</p>

<p>of all possible policies. We don’t want to maximize our reward by searching all possible policies,</p>

<p>instead we want to find a way to approximate p(x)H(x) by q(x), iteratively minimizing the distance</p>

<p>between them.</p>

<p><strong>Kullback-Leibler</strong> divergence</p>

<table>
  <tbody>
    <tr>
      <td>[KL(p_{1}(x)</td>
      <td> </td>
      <td>p_{2}(x))=E_{x-p_{1}(x)}\log \frac{p_{1}(x)}{p_{2}(x)}=E_{x-p_{1}}(x)[\log p_{1}(x)]-E_{x-p_{1}}(x)[\log p_{2}(x)]]</td>
    </tr>
  </tbody>
</table>

:ET