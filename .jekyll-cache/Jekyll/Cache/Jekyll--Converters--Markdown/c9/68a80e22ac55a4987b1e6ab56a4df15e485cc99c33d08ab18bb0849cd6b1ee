I"n<h2 id="actor-critic">Actor-critic</h2>

<p>减小方差策略梯度的方差,可以通过减掉一项baseline来达成, 这一项baseline可以是</p>

<p>一个episode中reward的均值, 其实更普遍地将, baseline这一项只要是状态相关的项</p>

<p>即可, 之所以是状态相关,为的是评价一个状态的好坏.</p>

<p>如果我们把一个状态下采取某个动作的好坏,看作两部分的叠加, 第一部分为一个状态</p>

<p>的好坏, 第二部分为在这个状态采用某个的哦你工作的好坏.</p>

\[Q(s,a)=V(s)+A(s,a)\]

<p>如果吧\(V(s)\) 这一项当作baseline, 则agent的policy gradient的系数就仅仅依赖\(A(s,a)\)了.</p>

<p>那么问题来了, \(V(s)\)的值我们并不知道, 既然不知道就用一个neural network来估计每一个</p>

<p>观测的\(s\)的\(V(s)\)吧!  其中\(A(s,a)\)这一项被称作是Advantage function.</p>

<p><span style="color : red">\(V(s)\)的更新是遵从bellman update的</span>.(We’ll carry out the Bellman step and then minimize</p>

<p>the mean square error to improve \(V(s)\) approximation)</p>

<p>所以类 actor-critic 方法可以看作是 value-based方法 和 policy-based方法的结合.</p>

<p><img src="../assets/img/2018-11-12/3.png" alt="" /></p>

<hr />

<p>当我们知道了每一个状态的\(V(s)\)时, 我们就可以计算策略网络(policy network)的梯度,用以</p>

<p>增加某个动作的概率(当某个动作的advantage function的值为正时)或是降低某个动作的概率</p>

<p>(当某个动作的advantage function的值为负时).</p>

<p>现在的网络有两个, 一个用来输出动作的概率, 这个叫做策略网络(policy network),也可以</p>

<p>称为actor, 它告诉我们应该输出什么动作, 另一个网络用来称为critic, 它告诉我们这个动作</p>

<p>有多好(多坏).</p>

<p>命名 Actor-Critic = Advantage Actor-Critic = A2C</p>

<p><strong>A2C(Advantage Actor Critic)</strong>的框架如下图所示:</p>

<p><img src="../assets/img/2018-11-12/a2c.jpg" alt="" /></p>

<p>实作的话, 一般两个网络会有公共部分, 主要是从计算的效率和收敛速度上来考虑的做法,</p>

<p>例如在玩atari的时候共享底层的卷积特征.</p>

<p>结构如下:</p>

<p><img src="../assets/img/2018-11-12/a2c_2.jpg" alt="" /></p>

<p><span style="color:red">从训练的角度来看, 完整的流程如下:</span></p>

<blockquote>
  <p>1 <strong>Initialize network parameters \(\theta\) with random values</strong></p>

  <p>2 <strong>Play \(N\) steps in the environment using the current policy \(\pi_{\theta}\),</strong></p>

  <p><strong>saving state \(s_{t}\) , action \(a_{t}\) ,  reward \(r_{t}\)</strong></p>

  <p>3 <strong>\(R=0\) or \(V_{\theta}(s_{t})=0\)  if the end of the episode is reached</strong></p>

  <p>4 For \(i=t-1...t_{start}\) (note that steps are processed backwards):</p>

  <ol>
    <li>
      <p><strong>Accumulate the PG</strong> \(\partial \theta_{\pi} \leftarrow \partial\theta_{\pi}+\nabla{\theta}\log\pi_{\theta}(a_{i} \|s_{i})(R-V_{\theta}(s_{i}))\)</p>
    </li>
    <li>
      <p><strong>Accumulate the value gradients</strong> \(\partial\theta_{v}\leftarrow \partial\theta_{v}+\frac{\partial(R-V_{\theta}(s_{i}))^2}{\partial\theta_{v}}\)</p>
    </li>
  </ol>

  <p>5 <strong>Update network parameters using the accumulated gradients, moving</strong></p>

  <p><strong>direction of PG \(\partial\theta_{\pi}\) (最大化)and in the opposite direction</strong></p>

  <p><strong>of the value gradient \(\partial\theta_{v}\)(最小化)</strong></p>

  <p>6 <strong>Repeat from step 2 until convergence is reached</strong></p>
</blockquote>

<p>但是实际应用中还会加上一些其他技巧:</p>

<ul>
  <li>
    <p>Entropy bonus is usually added to improve exploration. So Gradients accumulation</p>

    <p>is usually implemented as a loss function combining all three components: policy loss,</p>

    <p>value loss, and entropy loss. Careful the signs of these losses :smile:.</p>

    <p>add entropy value:</p>

    <p>\(L_{entropy} = \beta \sum_{i}\pi_{\theta}(s_{i})\log\pi_{\theta}(s_{i})\).</p>
  </li>
  <li>
    <p>To improve stability, it’s worth using several environments, our training batch wil be</p>

    <p>created from their observations. **这就是开多个环境一起跑的方法, 被称作Advantage   **</p>

    <p><strong>Asynchronous Actor-Critic, 简称A3C</strong>.</p>
  </li>
</ul>

:ET