---
layout: post
title: gym 函数与变量解读
date: 2018-10-23 18:00:00
img: aigame.png
tag: [programming]
---

### 1. Gym的组织结构与变量

***

 **gym**

\>envs

\>spaces

\>utils

\>warppers

core.py

error.py

***

#### 1.0. core.py

这里包含了很多基础类的定义例如: 

```python
class Env(object):
class Space(object):
class GoalEnv(object):
class Wrapper(Env):
class ObservationWrapper(Wrapper):
class RewardWrapper(Wrapper):
class ActionWrapper(Wrapper):
```

#### 1.1. 环境 (gym.envs)

这里主要包含里很多预先写好的环境, 例如**atari, box2d, cartpole** etc.

可以看一下 <span style="background-color: #00F00F">gym/envs/classic_control/cartpole.py</span>.

```python
class CartPoleEnv(gym.env):
    metadata = {
        'render.modes' : ['human', 'rgb_array'],
        'video.frames_per_second' : 50
 	}
    def __init__(self): pass
    def seed(self, seed=None): pass
    def step(self, action): pass 
    def reset(self): pass
    def render(self, mode='human'): pass
    def close(self): pass
```



#### 1.2. **空间 (gym.Space)**

```python
# In file core.py
class Space(object):
    """ Defines the observation and action spaces, so you can write generic code that 	  	  applies to any Env. For example, you can choose a random action. """
    def __init__(self, shape=None, dtype=None):
        import numpy as np # takes about 300-400ms to import, so we load lazily
        self.shape = None if shape is None else tuple(shape)
        self.dtype = None if dtype is None else np.dtype(dtype)
    
    def sample(self):
        """ Uniformly randomly sample a random element of this space """
        raise NotImplementedError
    
    def contains(self, x):
        """ Return boolean specifying if x is a valid
            member of this space """
        raise NotImplementedError
    
    __contains__ = contains
    
    def to_jsonable(self, sample_n):
        return sample_n
    def from_jsonable(self, sample_n):
        return sample_n
```

<span style="color:red">***env.action_space***</span>, <span style="color:red">***env.observation_space***</span>:  类型为 **Box**, **discrete** etc.均继承自 ***gym.Space***.

**Box** 类型 代表n-dimensional box, **Discrete** 类型 代表non-negative numbers.

### 1.3 封装(Wrapper)

继承自class Env.  经常用来包装一个环境. 为了增加额外的函数, 需要重新定义扩展的函数,例如step()和reset()等方法.

```python
class Wrapper(Env): 
class ObservationWrapper(Wrapper):
class RewardWrapper(Wrapper):
class ActionWrapper(Wrapper): 
class Monitor(Wrapper): # 常被用来保存录像
```

我们来看一下**ActionWrapper(Wrapper)**.

```python
class ActionWrapper(Wrapper):
    def step(self, action):
        action = self.action(action)
        return self.env.step(action)

    def reset(self):
        return self.env.reset()

    def action(self, action):
        deprecated_warn_once("%s doesn't implement 'action' method. Maybe it implements deprecated '_action' method." % type(self))
        return self._action(action)

    def reverse_action(self, action):
        deprecated_warn_once("%s doesn't implement 'reverse_action' method. Maybe it implements deprecated '_reverse_action' method." % type(self))
        return self._reverse_action(action)
```

改造一下

```python
class RandomActionWrapper(gym.ActionWrapper):
	def __init__(self, env, epsilon=0.1):
        super(RandomActionWrapper, self).__init__(env)
        self.epsilon = epsilon
        
    def action(self, action):
        if random.random() < self.epsilon:
            print("Random!")
            return self.env.action_space.sample()
        return action
```

如何使用呢?

```python
if __name__ == "__main__":
	env = RandomActionWrapper(gym.make("CartPole-v0"))
```

再看一下Monitor类.<span style="background-color: #00F00F">gym/wrappers/monitor.py</span>.

```python
if __name__ == "__main__":
	env = gym.make("CartPole-v0")
	env = gym.wrappers.Monitor(env, "recording")
```



## 2. 函数解读

**创建环境与重置环境**

```python
import gym
env = gym.make('CartPole-v0')
env.reset()
```

**执行动作**

```
action = env.action_space.sample()  # 随机的挑选一个动作
env.step(action)  # 与环境进行互动
```

<span style="color:red">***env.step()***</span> 

>**输入**: 动作的编号, 一般是离散的整数, 如 0, 1, 2
>
>**输出**: 4个量 
>
>1. **observation** (object): an environment-specific object representing your observation of the environment. For example, pixel data from a camera, joint angles and joint velocities of a robot, or the board state in a board game.   
>2. **reward** (float):  amount of reward achieved by the previous action. The scale varies between environments, but the goal is always to increase your total reward.   
>3. **done** (boolean):  whether it's time to <span style="color: blue">reset</span> the environment again. Most (but not all) tasks are divided up into well-defined episodes, and <span style="color:blue">done</span> being <span style="color:blue">True</span> indicates the episode has terminated. (For example, perhaps the pole tipped too far, or you lost your last life.)  
>4. **info** (dict): diagnostic information useful for debugging. It can sometimes be useful for learning (for example, it might contain the raw probabilities behind the environment's last state change). However, official evaluations of your agent are not allowed to use this for learning.  



