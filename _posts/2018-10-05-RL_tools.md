---
layout: post
title: PyTorch intro
date: 2018-10-05 10:00:00
img: aigame.png
tag: [machine learning]
---

## 1. Tensors

```python
import torch
import numpy as np
a = torch.tensor(3.14)  # a scalar is an zero-dimensional tensor
a = torch.FloatTensor(10) # vector with size 10
a = torch.FloatTensor(2, 3) # tensor with 2 rows and 3 cols
a = torch.FloatTensor(2,3,4) # a 3 dimensional tensor 

# Initialize a tensor with list or tuple
b = torch.FloatTensor([[1,2,3], [3,2,1]])
n = np.zeros(shape=(3,2))
b = torch.tensor(n) # torch.from_numpy(n) method will be deprecated

# GPU tensor
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
b = b.to(device) # if device is cuda
```

## 2. Gradients

**Static graph**: The graph gets processed and optimized by the DL library before any computation can be made.

**Dynamic graph**: The library records the order of operations performed, and when you ask it to calcute gradients, it unrolls its history of operations, accumulating the gradients of network parameters. This method is also called **notebook gradients**.

## 3. NN building blocks

```python
import torch.nn as nn
s = nn.Sequential(
        nn.Linear(2,5),
        nn.ReLU(),
        nn.Linear(5, 20),
        nn.Dropout(p=0.3),
        nn.Softmax(dim=1))
```

## 4. Custom layers

```python
class OurModule(nn.Module):
  def __init__(self. num_inputs, num_classes, dropout_prob=0.3):
    super(OurModule, self).__init__()
    self.pipe = nn.Sequential(
        nn.Linear(2,5),
        nn.ReLU(),
        nn.Linear(5, 20),
        nn.ReLU(),
        nn.Linear(20, num_classes),
        nn.Dropout(p=dropout_prob),
        nn.Softmax()
    )
  def forward(self, x):
    return self.pipe(x)

if __name__ == "__main__":
  net = OurModule(num_inputs=2, num_classes=3)
  v = torch.FloatTensor([[2,3]])
  out = net(v)
  print(net); print(out)
```

## 5. Loss functions

Pytorch 0.4 contains 17 different lossfunctions. For example:

* nn.MSELoss

* nn.BCELoss

* nn.CrossEntropyLoss

  â€‹

## 6. Optimizers

In the torch.optim package, PyTorch provides lots of popular optimizer implementations and 

the most widely known are as follows:

* SGD: A vanilla stochastic gradient descent algorithm with optional momentum extension
* RMSprop: An optimizer, proposed by G. Hinton
* Adagrad: An adaptive gradients optimizer

## 7. Monitoring with TensorBoard

pip install tensorboard-pytorch

***

