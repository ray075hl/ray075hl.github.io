## 基本概念与符号

#### *策略*

策略是静态的，与时间无关，是一个概率（在状态S下执行动作a的概率）

$\Sigma(a|s) = p[A_t=a|S_t=s] \tag {1}$



### *奖励函数*

即在给定策略 $\pi$ 时的累积回报。

$$G_t=\sum_{k=0}^{\infty}r^kR_{t+k+1}\tag  {2}$$



#### *状态-值函数*

给定策略 $\pi$ ，累积回报在状态s处的期望值 定义为状态-值函数。

$$v_{\pi}(s)=E_{\pi}[\sum_{k=0}^{\infty}r^kR_{t+k+1}|S_t=s] \tag         {3}$$



#### *行为-值函数*

给定策略 $\pi$ ，累积回报在状态s处执行动作a的期望值 定义为行为-值函数

$$q_\pi(s,a)=E_\pi[\sum_{k=0}^{\infty}r^kR_{t+k+1}|S_t=s,A_t=a] \tag   {4}$$



#### *状态-值函数的贝尔曼方程*

$$
\begin{split} 
v(s)&=E[G_t|S_t=s] \\ 
&=E[R_{t+1}+rR_{t+2}+...|S_t=s] \\ 
&=E[R_{t+1}+r(R_{t+2}+rR_{t+3}+...)|S_t=s] \\
&=E[R_{t+1}+rG_{t+1}|S_{t}=s] \\
&=E[R_{t+1}+rv(S_{t+1})|S_t=s] 
\end{split}\tag   {5}
$$
#### *状态-动作值函数的贝尔曼方程*

$q_{\pi}(s,a)=E_{\pi}[R_{t+1}+rq(S_{t+1},A_{t+1})|S_t=s,A_t=a] \tag   {6}$

贝尔曼方程的意义在于，把问题的求解转化为递归的求解其子问题，这是运用动态规划的必要条件。











