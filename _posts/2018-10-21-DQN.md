---
layout: post
img: aigame.png
date: 2018-10-21 12:00:00
tag: [machine learning, algorithm]
title: DQN(deep Q-learning)
---

###  值迭代方法存在的问题

**值迭代**或是**Q值迭代**或是**策略迭代**需要遍历***所有***的状态(**states**),针对每一个状态做贝尔曼更新**(bellman approximation)**.

* 第一个明显的问题是: 环境状态的数目有时候是特别巨大的,假设状态是一张$$100*100$$的彩色图片,那么所有可能的状态数目为$$100*100*3*256=7680000$$, 对如此庞大的数目建立一个**Q-table**显然十分浪费存储以及密集的计算.最重要的是针对某个特定问题(例如玩游戏的屏幕$$100*100$$),7680000中的绝大多数情况可能是永远都不会出现的,遍历是没有必要的.
* 第二个问题是:值迭代方法的动作空间必须是离散的,的确,$$Q(s,a), V(s)$$ 估计假定我们的动作空间是互斥的离散集合. 这个问题比上一个问题更加具有挑战性.

#### 对于第一个问题(状态空间数目庞大)

对于没有出现过的状态,只针对出现过的状态进行value的计算,这就是**Q-learning**.

As mentioned earlier, and for case with explicit state-to-value mappings, has the following steps:

>1. Start with an **empty table**, mapping states to values of actions.
>
>2. By interacting with the environment, **obtain the tuple $$s, a, r, s'$$  (state, action, reward, and the new state)**. In this step, **we need to decide which action to take**, and there is no single proper way to make this decision. We discussed this problem as exploration versus exploitation and will talk a lot about this.
>
>3. Update the $$Q(s, a)$$ value using the Bellman approximation:
>
>   $$Q(s,a) \leftarrow r + \gamma \max_{a' \in A}Q(s',a')$$ 
>
>   **Or** using "blending" technique, which is just averaging between old and new values of Q using learning rate $$\alpha$$. Allows values of $$Q$$ to converge smoothly.
>
>   $$Q(s,a) \leftarrow (1-\alpha)Q(s,a) + \alpha(r + \gamma \max_{a' \in A}Q(s',a'))$$ 
>
>4. Repeat from step 2.

这就是所谓的**表格Q-learning**, 它不同于Q值迭代的地方在于,没有显式的遍历所有的状态,只针对出现过的状态进行值更新.它能够处理可观测状态较多的情形,但是能力有限,它还是无法处理图像这种状态量极其巨大的情形,还有诸如CartPole这种可观测状态近乎无穷的问题(因为环境的输出是浮点数).



### DQN

我们可以用一个非线性的映射来代替Q-table, 

$$Q(s, a) \rightarrow value$$

这个非线性映射就是Neural Network,

$$NN(s, a) \rightarrow value$$

一个简单的改进为:

> 1. Initialize $$Q(s,a)$$ with some initial approximation
>
> 2. By interacting with the environment, obtain the tuple$$(s,a,r,s')$$ 
>
> 3. Calculate loss: $$L = (Q(s,a) - r)^2$$  if episode has ended or 
>
>    $$L = Q(s,a)-(r+\gamma\max_{a' \in A}Q(s',a'))^2$$ otherwise
>
> 4. Update $$Q(s,a)$$ using the **stochastic gradient descent(SGD)** algorithm, by
>
>    minimizing the loss with respect to the model parameters
>
> 5. Repeat from step 2 until converged

上面的算法看起来很自然,不过存在很多问题,并不能有效的工作.



**问题有以下几点:**

这几个问题比较有意思, 后续再专门详细讨论.

1. Exploration versus exploitation

   探索与利用的矛盾是强化学习中的关键问题

2. Stochastic gradient descent(SGD) optimization

   主要是SGD的优化针对的是i.i.d类型的数据,而此处的数据分布在不断地变化

3. Correlation between steps

   解决方法中的一个为 target network

4. The Markov property

   解决方法中的一个为 采用连续的数据堆作为输入(如连续的几帧图像)



***The final form of DQN training***(with target network)

from paper [Playing Atari with Deep Reinforcement Learning](https://www.cs.toronto.edu/~vmnih/docs/dqn.pdf).(without target network)

>1. Initialize parameters for $$Q(s,a)$$ and $$\hat{Q}(s,a)$$ with random weights, $$\epsilon \leftarrow 1.0$$, and empty replay buffer
>2. With probability $$\epsilon$$, select a random action a, otherwise $$a = \arg \max_{a} Q(s,a)$$ 
>3. Execute action a in an emulator and observe reward $$r$$ and the next state $$s'$$
>4. Store transition $$(s, a, r, s')$$ in the replay buffer
>5. Sample a random mini-batch of transitions from the replay buffer
>6. For every transition in the buffer, calculate target $$y=r$$ if the episode has ended at this step or $$y=r+\gamma \max_{a'\in A}\hat{Q}(s',a')$$ otherwise
>7. Calculate loss: $$L=(Q(s,a)-y)^2$$
>8. Update $$Q(s,a)$$ using the SGD algorithm by minimizing the loss in respect to model parameters
>9. Every N steps copy weights from $$Q$$ to $$\hat{Q}_{t}$$
>10. Repeat from step 2 until converged

